import warnings
warnings.filterwarnings('ignore')

#Import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import scipy
from pandas_datareader import wb
from datetime import datetime
import matplotlib.pyplot as plt


# **LINEAR REGRESSION**

## **¿What is linear regression?**

<font size="3"> Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x). The equation for the 1 dimensional case is: 
    
</font>  <br><br>

<font size="5"> 

$y = \beta _0 + \beta _1 * x$  

</font>  <br>


<font size="3"> Let's suppose we have two variables x, y with the following values: </font>  <br>


<font size="5">
    
| x  | y |
| -- | -- |
| 5.3  | 5.8  |
| 1.1  | 1.5  |
| ...  | ...  |
| 7.8  | 8.3  |

</font>  <br>

<font size="3"> If we plot these points we get: </font>  <br>

![This is an image](Nube_puntos.png)



<font size="3"> The straight line shows the relationship between both variables using the following equation: </font>  <br>

<div class="alert alert-block alert-info">
<font size="5"> 
$$y _i = \beta _0 + \beta _1 * x _i + error$$ 
</font>  
</div><br>

<font size="3"> In this case one could say that Y depends on X (or viceversa) and that there is a linear relationship between both. That statement should be proven though. It is important to note, that two variables can be closely related and still not have a linear relationship. In those cases, linear regression is not a good modeling choice at first glance. However, one can transform those variables and maybe then, they will have a linear relationship. Some transformation that are usually used are the logarithm or power laws.  What we are saying here is that the term "linear regression" is used to specify models that are linear in tha parameters $\beta _0$ and $\beta _1$.
</font>  <br>


<font size="3"> ¿How do you get that straight line and its parameters? The most used method is called "Ordinary Least Squares" (O.L.S) y is shown in the chart below:
</font>  <br>



![This is an image](Linea_regresión.png)


<font size="3"> The methodology calculates the difference between each point and the one suggested by the line. We should then get the square of those differences and add them all. We should look for the parameters that minimice that difference and those would be the O.L.S parameters for the straight line. 
</font>  <br>



## **¿What kind of data should we use to do linear regression?**


<font size="3"> We normally use cross sectional data. That means, that all the data have been collected in a single moment. Unfortunatelly in finance, most of the data come from a time series. That doesn't mean we can't use linear regression for data series, but that we need to be careful when applying this technique under such circumstances. As we are only getting familiar with linear regression, we will start with cross sectional data.<br>     

We can use the World Bank database, which contains a lot information on differente variables. You could have heard that education is one of the most important determinants for economic growth. Thus, a reasonable question to explore if the amount of money that a government spends on education has any relationship with its GDP per capita. 
</font>  <br>

<div class="alert alert-block alert-warning">
<font size="3"> 
- X: Education spending as % of GDP<br>
- Y: GDP per capita
</font>  
</div><br>
</font>

<font size="3">
Now we will learn how to get the data using pandas_datareader library in python:
</font



## **¿Do we have a linear relationship here?**<br>

<font size="3">
A linear relationship is not easily observerd. We can use the python library statsmodel to run Linear Regression. That library has the most useful statistics to analyze this data modelling technique. Let's watch:
</font><br>



## **¿What does all that data mean?**<br>

<font size="3">
We got a lot of information when we ran our linear regression. Let's focus only on the most important: The coefficients (the estimates of  $\beta _0$ and $\beta _1$ and ), as well as their -$p$ and -$R$ squared values.
</font><br><br>


**Coefficients**<br><br>

<font size="3">
The intercept  is about 7352 USD. This can be thought of as the baseline GDP per Capita. (Frequently, the intercept does not have a meaningful interpretation – that is okay.) The slope (the coefficient  for the Education Spending) is 1599.87. The interpretation of this coefficient is this: if a country spends 1% more of its GDP on education , its GDP per capita is expected to increase by 1599.87 USD on average.  
 </font><br>
 
 **p-values**<br><br>

<font size="3">
The null hypothesis we are testing here is <br><br>   
$H _0 : \beta _1 = 0$<br><br>
and the alternative is<br><br>
$H _1 : \beta _1 \neq 0$<br><br>  
    
The $p$-value of $ \beta _1$  (given under the column: $P > \vert t \vert $ ) is 0.127. Thus, it is not statistically significant at the 0.05 level, and we can not reject the null hypothesis. This implies that education spending as GDP percentage does explain the GDP per capita of a country.

There is no perfect or exact way of setting a statisically significant  - value threshold. It really depends on your needs as a data professional.  - values are used to assess how accurate we think a specific coefficient is. How certain do you need your interpretation of your coefficients to be? Generally, 0.05 is a common heuristic but it is not a hard-line number. 
</font><br><br>
 
 **R-squared**<br><br>


<font size="3"> 
One of the key quantities that should be paid attention to while interpreting a regression table is the quantity $R$-squared. Note that the table shows $R$-squared and adjusted $R$-squared. We will focus on $R$-squared. This quantity is always going to be between 0 and 1. For the GDP per Capita vs. Education Spending model, this quantity is 0.019 = 1.9%. In the world, there is a large variation in GDP per Capita. This means different countries have different GDP per Capita.

An $R$-squared of 1.9% in this linear model means that this observed variation in GDP per Capita is mostly due to random chance; we should probably look for another variable with higher explanatory powe. The higher the $R$-squared, the higher the percentage of observed variation that can be explained by the model. Since model1 only explains about 1.9% of the variation, this motivates us to investigate if factors other than Education Spending can be used to explain the GDP per Capita differences. Or if we should transform the variable Education Spending and see if it has a more powerful explanation of the variation on the GDP per capita. Let's do that with a logarithmic transformation:
</font><br><br>



<font size="3">
That was a very small improvement. <br><br>We should look for other variables or explore another problem. Let's see if GDP per capita has any relationship with life expectancy. You could probably think, that the richer the country the longer people live. Let's explore that problem:
</font><br><br>



<font size="3">
Now the relationship looks much more linear than before. This looks promising! <br><br> Let's jump into modelling these variables immediately using linear regression.
</font><br><br>


<font size="3">
$R$-squared is much higher now around 71.5%.<br><br>

The $p$-value of $ \beta _1$  is 0.00. Thus, it is statistically significant at the 0.05 level, and we can reject the null hypothesis. This implies that Log of GDP per Capita does explain the Life Expectancy of the inhabitants of a country.<br><br>

Let's dig a little deeper into the meaning of $R$-squared. Graphically you can say that $R$-squared shows how much better does the linear regression model explain the data, when compared to a model of the average. 

Recall that the formula for $R$-squared is the following:

$R^2= 1 - \frac{RSS}{TSS} $  

 
Here RSS and TSS denote the Residual and Total Sum of Squares respectively. To understand the above formula carefully, look at the chart below where we plotted Life Expectancy vs. Log of GDP per Capita again.


If you consider a naive model given by:<br><br>


<div class="alert alert-block alert-info">
<font size="5"> 
$$Life Expectancy = \beta _0  + error$$ 
</font>  
</div><br>

the best guess for $\beta _0$ is just the average Life Expectancy. This is given by the red line. The sum of the squares of the residuals here is called the Total Sum of Squares (TSS). R-squared measures how well the regression line of model3 (Life Expectancy vs. Log of GDP per Capita) given by the blue line, explains the observed variation as compared to the naive model. The sum of residual squares for this model is the RSS.

</font><br>



<font size="3"> 

So far we have followed an example of linear regression without asking if there are some assumptions we need to check. Indeed there are a few. Some of the most important assumptions are:<br><br>

***
1. The mean value of the residuals is cero.
2. The variance of the residuals is constant.
3. The residuals are not auto-correlated.
***

In the classic linear regression, one assumption is that residuals are normal and indepently distributed. If that happens 1, 2 and 3 are ok.
<br><br>


In the following chart, we can check if the mean of the residuals is zero from visual inspection.
</font><br>


<font size="3"> 
The mean seems to be around zero, but there is some skew in the data. Let's plot an histogram an watch if they look somehow normally distributed.
</font><br>


<font size="3"> 
One can plot the residuals using a probability plot as well. If the residuals are normally distributed they will perfectly fit on the red line in the following chart.
</font><br>

<font size="3"> 
To finish this topic we will show a way to identify which variables might me transformed, as we did we the logarithmic GDP per capita in this example. You can use the `boxcox statistics`. If this number is close to 1 you don't need to do anything. When it is close to zero, the logarithmic transformation is recomended. When it is closer to 0.5 you can use the square root transformation. When is closer to zero or negative, logarithmic transformation is a good choice.
</font>

<font size="3"> 
<div class="alert alert-block alert-danger">
<b>Please remember:</b> Here the boxcox statistics is close to zero. It suggest a logarithmic transformation. We can corroborate that transformation using the probability plot we introduce before. 
</div>
</font>

<font size="3"> 
<div class="alert alert-block alert-success">
<b>ONE LAST THING:</b> To finish this topic about linear regression, we would like to show another library that handles these calculations. It is sklearn. Below we show and example on how to do linear regression using this package.
</div>
</font>


> What we have to learn to do, we learn by doing. 

Aristotle


posts about quantitative finance, data science and emerging markets  <a href="http://earthdatascience.org" target="_blank">this link</a>.
